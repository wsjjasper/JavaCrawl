In Apache Airflow, I have a requirement to setup dependency between 2 parent DAG(DAG_P1 and DAG_P2) and 2 children DAG(DAG_C1 and DAG_C2). DAG_P1 will generate dataset D1 with dataset key, DAG_P2 will generate dataset D2 with another dataset key. Both children DAG should be triggered only when both DAG_P1 and DAG_P2 generated dataset with same key. For example, DAG_P1 generate dataset with key k1, but DAG_P2 generated datset with key k2, in this case, the children DAG should not be triggered, but if DAG_P2 generated dataset with key k1, then children DAG should be triggered. 
In Apache Airflow, you can achieve this by using cross-DAG dependencies with ExternalTaskSensor, along with custom logic to check the dataset keys in your child DAGs. Here's a step-by-step guide to setting up the required dependencies:

First, create your parent DAGs (DAG_P1 and DAG_P2). Make sure that each DAG generates the dataset and writes the dataset key to a shared storage or database so that it can be accessed by the child DAGs. It's important to choose a storage system that allows concurrent access, such as a distributed file system or a database.

In each child DAG (DAG_C1 and DAG_C2), add an ExternalTaskSensor task for each parent DAG. These sensors will wait for the parent DAGs to complete before proceeding.

python
Copy code
from airflow import DAG
from airflow.sensors.external_task_sensor import ExternalTaskSensor

DAG_C1 = DAG('DAG_C1', ...)

wait_for_P1 = ExternalTaskSensor(
    task_id='wait_for_P1',
    external_dag_id='DAG_P1',
    execution_delta=None,
    mode='poke',
    dag=DAG_C1
)

wait_for_P2 = ExternalTaskSensor(
    task_id='wait_for_P2',
    external_dag_id='DAG_P2',
    execution_delta=None,
    mode='poke',
    dag=DAG_C1
)
After setting up the ExternalTaskSensor tasks, create a custom Python callable that checks if the dataset keys generated by DAG_P1 and DAG_P2 match. You can do this by reading the dataset keys from the shared storage or database.
python
Copy code
def check_keys_match(**kwargs):
    # Implement your logic to check if the dataset keys match
    # Retrieve the dataset keys from the shared storage or database
    key_P1 = ...
    key_P2 = ...

    if key_P1 == key_P2:
        return True
    else:
        return False
Create a ShortCircuitOperator task in each child DAG that uses the custom Python callable to decide whether the child DAG should continue execution or not.
python
Copy code
from airflow.operators.python_operator import ShortCircuitOperator

check_keys = ShortCircuitOperator(
    task_id='check_keys',
    python_callable=check_keys_match,
    provide_context=True,
    dag=DAG_C1
)
Finally, set up the dependencies between the tasks in each child DAG.
python
Copy code
wait_for_P1 >> wait_for_P2 >> check_keys
Repeat the same process for DAG_C2. With this setup, DAG_C1 and DAG_C2 will only proceed with their execution if both DAG_P1 and DAG_P2 have completed, and the dataset keys generated by them match.




Jasper Wsj
In your setup, does it support concurrent runs? Let's say multipe DAG_P1 and DAG_P2 instances being triggered, and generated datasets with different keys. Some of them have same keys, so I assume multiple downstream DAG_C1 and DAG_C2 instances will be triggered at same time, and children DAG may be running in parallel , is that correct?
Yes, my suggested setup supports concurrent runs. When multiple instances of DAG_P1 and DAG_P2 are triggered and generate datasets with different keys, the corresponding instances of DAG_C1 and DAG_C2 will be triggered based on the matching dataset keys. Multiple instances of the child DAGs can run in parallel if their corresponding parent DAG instances have completed with matching dataset keys.

To ensure that the child DAG instances are triggered with the correct dataset keys, you should store the dataset keys along with their respective run timestamps or execution dates in your shared storage or database. This way, you can look up the dataset keys based on the execution dates of the parent DAG instances when checking for key matches in the child DAGs.

Here's an example of how you can modify the check_keys_match function to check for dataset keys based on the execution dates:

python
Copy code
def check_keys_match(**kwargs):
    ti = kwargs['ti']
    execution_date = ti.execution_date

    # Implement your logic to retrieve the dataset keys for the specified execution_date
    key_P1 = ...
    key_P2 = ...

    if key_P1 == key_P2:
        return True
    else:
        return False
With this setup, multiple instances of DAG_C1 and DAG_C2 can run concurrently based on the dataset keys generated by the concurrently running instances of DAG_P1 and DAG_P2.
